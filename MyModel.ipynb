{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/focusondoingonething/mypapercode/blob/main/MyModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFrIIvRAA14a"
      },
      "source": [
        "#Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS19H45V6nnY"
      },
      "source": [
        "'readme':\n",
        "\n",
        "1) This is the code of paper: \"An Improved Graph Neural Network Frame for Text Classification\".\n",
        "\n",
        "2) Open this in colab platform and click the \"connect\" and run to reproduce the result (replace the dataset file path).\n",
        "\n",
        "3) The original training set and test set have been determined, so k-fold cross-validation is not appicable. This experiment uses 5 times of shuffle, and runs 5 times after each shuffle to take the average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd006pVA_cik"
      },
      "source": [
        "Import or Install Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEISbHgrQbjH"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch as th\n",
        "from sklearn.model_selection import train_test_split  # for train and dev set split\n",
        "from torch.nn import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Install and Import Libraries\n",
        "try:\n",
        "  import word2vec\n",
        "except ModuleNotFoundError:\n",
        "  !pip install word2vec\n",
        "  import word2vec\n",
        "\n",
        "try:\n",
        "  import nnsplit\n",
        "except ModuleNotFoundError:\n",
        "  !pip install nnsplit\n",
        "\n",
        "try:\n",
        "  import torch_scatter\n",
        "except ModuleNotFoundError:\n",
        "  TORCH = th.__version__.split('+')[0]\n",
        "  CUDA = 'cu' + th.version.cuda.replace('.','')\n",
        "  !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "try:\n",
        "  import dgl\n",
        "except ModuleNotFoundError:\n",
        "  CUDA = 'cu' + th.version.cuda.replace('.','')\n",
        "  !pip install dgl-{CUDA} -f https://data.dgl.ai/wheels/repo.html\n",
        "  import dgl\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from dgl.nn import GATConv\n",
        "\n",
        "\n",
        "# Set Random Seed\n",
        "SEED = 42\n",
        "th.manual_seed(SEED)\n",
        "th.cuda.manual_seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA5UdbT0GZj2"
      },
      "source": [
        "#Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clean Text"
      ],
      "metadata": {
        "id": "yrvbv8dKHOYH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DltPNGzY72Fl"
      },
      "source": [
        "Just run it once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfza6vqdMITO"
      },
      "outputs": [],
      "source": [
        "# Clean Data\n",
        "def Clean_Text(text):\n",
        "  text = text.lower()\n",
        "  text = text.strip()\n",
        "  text = re.sub(r\"\\'s\", \" is\", text)\n",
        "  text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "  text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "  text = re.sub(r\"\\'re\", \" are\", text)\n",
        "  text = re.sub(r\"\\'d\", \" would\", text)\n",
        "  text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "\n",
        "  text = re.sub(r\"[^A-Za-z().?!\\'\\`]\", \" \", text)\n",
        "  text = re.sub(r\"\\(\", \"\", text)\n",
        "  text = re.sub(r\"\\)\", \"\", text)\n",
        "  text = re.sub(r\"\\'\", \"\", text)\n",
        "  text = re.sub(r'`', '', text)\n",
        "  text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "  return text\n",
        "\n",
        "def Clean_and_Save(path, dataset_name):\n",
        "  NAME = dataset_name\n",
        "  if NAME not in ['20ng', 'r8', 'r52', 'oh', 'mr']:\n",
        "    raise ValueError('The dataset is not support')\n",
        "  raw_x = []\n",
        "  with open(os.path.join(path, NAME+'.txt'), encoding='latin1') as f:\n",
        "    data = f.readlines()\n",
        "    print(f'{NAME}, Total sample: {len(data)}')\n",
        "  with open(os.path.join(path, NAME+'_mapping.txt'), encoding='latin1') as f:\n",
        "    map = f.readlines()\n",
        "    print(f'{NAME}, Total mapping: {len(map)}')\n",
        "  if len(map) != len(data):\n",
        "    raise Exception('Map size not equal to data')\n",
        "  \n",
        "  # Clean Sample and Save\n",
        "  tra_x, tra_y = [], []\n",
        "  tes_x, tes_y = [], []\n",
        "  for i in range(len(map)):\n",
        "    cle_x = Clean_Text(data[i]).strip()  # X\n",
        "    lab = map[i].split('\\t')  # y\n",
        "    if lab[len(lab)-2] in ['20news-bydate-test', 'test']:\n",
        "      tes_x.append(cle_x)\n",
        "      tes_y.append(re.sub(r'\\n', '', lab[len(lab)-1]))\n",
        "    elif lab[len(lab)-2] in ['20news-bydate-train', 'train', 'training']:\n",
        "      tra_x.append(cle_x)\n",
        "      tra_y.append(re.sub(r'\\n', '', lab[len(lab)-1]))\n",
        "  if len(tes_y)!=len(tes_x) or len(tra_x)!=len(tra_y):\n",
        "    raise Exception('Numbef of x is not equal y')\n",
        "  \n",
        "  print(f'Total train: {len(tra_x)}')\n",
        "  print(f'Total test: {len(tes_x)}')\n",
        "\n",
        "  # Save to csv\n",
        "  df_tra = pd.DataFrame({'tra_y': tra_y, 'tra_x': tra_x})\n",
        "  df_tes = pd.DataFrame({'tes_y': tes_y, 'tes_x': tes_x})\n",
        "  # Remove the csv header\n",
        "  df_tra.columns = range(df_tra.shape[1])\n",
        "  df_tes.columns = range(df_tes.shape[1])\n",
        "  df_tra.to_csv(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/data/{NAME}-stemmed.txt', index=False, sep='\\t',header=None)\n",
        "  df_tes.to_csv(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/data/{NAME}-test-stemmed.txt', index=False, sep='\\t',header=None)\n",
        "\n",
        "path = '/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/data/'\n",
        "for name in ['20ng', 'r8', 'r52', 'oh', 'mr']:\n",
        "  t0 = time.time()\n",
        "  #if name not in [\"oh\"]: continue  # Check for specific dataset \n",
        "  Clean_and_Save(path, name)\n",
        "  print(f'Time: {time.time() - t0:.4f}s')\n",
        "  print('='*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert to Token"
      ],
      "metadata": {
        "id": "teoAXc2hHVsy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxTtmxTqGROo"
      },
      "source": [
        "Text to Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HevAZHVIQ9Lr"
      },
      "outputs": [],
      "source": [
        "# Build Vocab on All Train Sample\n",
        "def BuildVocab(TrainSample, min_count=5):\n",
        "  freq = {}\n",
        "  for x in TrainSample:\n",
        "    for t in word_tokenize(x):\n",
        "      if t not in freq:\n",
        "        freq[t] = 0\n",
        "      else:\n",
        "        freq[t] += 1\n",
        "  del_key = []\n",
        "  for i in freq:\n",
        "    if freq[i]<min_count:\n",
        "      del_key.append(i)\n",
        "  \n",
        "  for i in del_key:freq.pop(i)\n",
        "\n",
        "  vocab_id = {}\n",
        "  for i, key in enumerate(freq): vocab_id[key] = i\n",
        "  vocab_id[\"unk\"] = i+1  # Unknow word index (OOV)\n",
        "\n",
        "  print(f'vocab size: {len(vocab_id)}')\n",
        "  print('='*50)\n",
        "\n",
        "  return vocab_id\n",
        "\n",
        "\n",
        "# Convert Word to ID Number\n",
        "def X2Id(X, vocab):\n",
        "  ids = []\n",
        "  for w in word_tokenize(X):\n",
        "    if w in vocab:\n",
        "      ids.append(vocab[w])\n",
        "    else:\n",
        "      ids.append(vocab[\"unk\"])\n",
        "  \n",
        "  return ids\n",
        "\n",
        "\n",
        "# Convert String Tex to Token Number\n",
        "def Text2Token(vocab, text_list):\n",
        "  temp = []\n",
        "  for text in text_list:\n",
        "    temp.append(X2Id(text, vocab))\n",
        "  \n",
        "  return temp\n",
        "\n",
        "\n",
        "# Split Train and Dev Set\n",
        "def Tra_Dev_Set(all_train_text, all_train_label, dataset_name):\n",
        "  if dataset_name==\"r52\":\n",
        "    train_x, dev_x, train_y, dev_y = train_test_split(all_train_text, all_train_label, test_size=0.06, shuffle=True, random_state=42)\n",
        "  else:\n",
        "    train_x, dev_x, train_y, dev_y = train_test_split(all_train_text, all_train_label, \\\n",
        "                                                      test_size=0.1, shuffle=True, random_state=42, stratify=all_train_label)\n",
        "\n",
        "  return train_x, dev_x, train_y, dev_y\n",
        "\n",
        "\n",
        "# Shuffle All Train Sample, Split and Token\n",
        "def MyDataset(dataset_name):\n",
        "  NAME = dataset_name\n",
        "  if NAME not in ['20ng', 'r8', 'r52', 'oh', 'mr']:\n",
        "    raise ValueError('The dataset is not support')\n",
        "  \n",
        "  frame = {\"all_train_text\":[], \"all_train_label\":[], \"test_text\":[], \"test_label\":[]}\n",
        "  PATH = '/content/data/'\n",
        "  for t1,t2,t3 in [(\"-stemmed.txt\", \"all_train_text\", \"all_train_label\"),\n",
        "                      (\"-test-stemmed.txt\", \"test_text\", \"test_label\")]:\n",
        "    with open(os.path.join(PATH, NAME+t1), 'r') as f:\n",
        "      data = f.readlines()\n",
        "      for line in data:\n",
        "        line = line.strip()\n",
        "        temp = line.split('\\t')\n",
        "        frame[t2].append(temp[1])  # X\n",
        "        frame[t3].append(temp[0])  # y\n",
        "    \n",
        "  train_text, dev_text, train_label, dev_label = Tra_Dev_Set(frame[\"all_train_text\"], frame[\"all_train_label\"], NAME)\n",
        "  test_text, test_label = frame[\"test_text\"], frame[\"test_label\"]\n",
        "  \n",
        "  num_class = list(set(train_label))  # len(num_class)\n",
        "  label2idx = {label: idx for idx, label in enumerate(num_class)}\n",
        "  print(f'Dataset={NAME}, Num_class={len(num_class)}, labels={label2idx}')\n",
        "  print(f'All_train={len(frame[\"all_train_text\"])}, Train_size={len(train_text)}, Dev_size={len(dev_text)}, Test_size={len(test_text)}')\n",
        "\n",
        "  vocab = BuildVocab(train_text)\n",
        "  train_text_tok = Text2Token(vocab, train_text)\n",
        "  dev_text_tok = Text2Token(vocab, dev_text)\n",
        "  test_text_tok = Text2Token(vocab, test_text)\n",
        "\n",
        "  train_label_tok = [label2idx[t] for t in train_label]\n",
        "  dev_label_tok = [label2idx[t] for t in dev_label]\n",
        "  test_label_tok = [label2idx[t] for t in test_label]\n",
        "\n",
        "  \n",
        "  return train_text_tok,  train_label_tok, dev_text_tok, dev_label_tok, \\\n",
        "          test_text_tok, test_label_tok, len(num_class), vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRSv-iMn_7-B"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aLY9tF9YmZW"
      },
      "outputs": [],
      "source": [
        "# Model Architecture\n",
        "class GATLayer(th.nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim, num_class):\n",
        "    super(GATLayer, self).__init__()\n",
        "    self.num_layer = 3\n",
        "    self.heads = [3]*(self.num_layer-1) + [1]\n",
        "    self.feat_drop = 0.4\n",
        "    self.attn_drop = 0.4\n",
        "    self.neg_slope = 0.2\n",
        "    self.act = F.elu\n",
        "    self.gat_layers = th.nn.ModuleList()\n",
        "\n",
        "    if self.num_layer > 1:\n",
        "      self.gat_layers.append(GATConv(in_dim, hidden_dim, self.heads[0], self.feat_drop, \\\n",
        "                                    self.attn_drop, self.neg_slope, False, self.act))\n",
        "      for l in range(1, self.num_layer-1):\n",
        "        self.gat_layers.append(GATConv(hidden_dim *self. heads[l-1], hidden_dim, self.heads[l], \\\n",
        "                                      self.feat_drop, self.attn_drop, self.neg_slope, False, self.act))\n",
        "      self.gat_layers.append(GATConv(hidden_dim * self.heads[-2], num_class, \\\n",
        "                                    self.heads[-1], self.feat_drop, self.attn_drop, \\\n",
        "                                    self.neg_slope, False, None))\n",
        "    else:\n",
        "      self.gat_layers.append(GATConv(in_dim, num_class, self.heads[0], self.feat_drop, \\\n",
        "                                    self.attn_drop, self.neg_slope, False, None))\n",
        "\n",
        "  def forward(self, g, inputs):\n",
        "    h = inputs\n",
        "    for l in range(self.num_layer):\n",
        "      h = self.gat_layers[l](g, h)\n",
        "      h = h.flatten(1) if l != self.num_layer - 1 else h.mean(1)\n",
        "    \n",
        "    return h\n",
        "\n",
        "\n",
        "class MyModel(th.nn.Module):\n",
        "  def __init__(self, vocab, in_dim, hidden_dim, num_class, device):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.gram = 3\n",
        "    self.vocab = vocab\n",
        "    self.num_class = num_class\n",
        "    self.device = device\n",
        "    self.max_length = 300\n",
        "    self.vocab_size = len(vocab)\n",
        "    self.node_hidden = th.nn.Embedding(self.vocab_size, in_dim)\n",
        "    self.node_hidden.weight.data.copy_(th.tensor(self.Load_w2v('/content/glove.6B.300d.w2vformat.txt')))\n",
        "    self.node_hidden.weight.requires_grad = True\n",
        "    self.gat = GATLayer(in_dim, hidden_dim, num_class)\n",
        "  \n",
        "  def Load_w2v(self, path):\n",
        "    w2v = word2vec.load(path)\n",
        "    embedding_matrix = []\n",
        "    unk_d = len(w2v['the'])  # Unknow eord dimension\n",
        "    for word in self.vocab:\n",
        "      try:\n",
        "        embedding_matrix.append(w2v[word])\n",
        "      except KeyError:\n",
        "        embedding_matrix.append(np.zeros(unk_d))\n",
        "    \n",
        "    return np.array(embedding_matrix)\n",
        "  \n",
        "  def AddEdges(self, sample, local_vocab_id): \n",
        "    edges = []\n",
        "    for i, src in enumerate(sample):\n",
        "      for j in range(max(0, i-self.gram), min(i + self.gram + 1, len(sample))):  # Undirected graph\n",
        "        dst = sample[j]\n",
        "        edges.append([local_vocab_id[src], local_vocab_id[dst]])\n",
        "    \n",
        "    return edges\n",
        "\n",
        "  def BuildGraph(self, sample):\n",
        "    t = len(sample)\n",
        "    if t == 0: raise Exception('sample length is equal 0')\n",
        "    if t > self.max_length: sample = sample[:self.max_length]\n",
        "    local_vocab = set(sample)\n",
        "    n = len(local_vocab)\n",
        "    local_vocab_id = dict(zip(local_vocab, range(n)))\n",
        "    u, v = zip(*self.AddEdges(sample, local_vocab_id))\n",
        "    g = dgl.graph((u, v), num_nodes=n).to(self.device)\n",
        "    local_vocab_embedding = th.tensor(list(local_vocab)).to(self.device)\n",
        "    g.ndata['h'] =  self.node_hidden(local_vocab_embedding)\n",
        "\n",
        "    return g\n",
        "  \n",
        "  def forward(self, sample):\n",
        "    one_batch_g = []\n",
        "    for t in sample:\n",
        "      one_batch_g.append(self.BuildGraph(t))  # One batch size graphs\n",
        "    \n",
        "    one_g = dgl.batch(one_batch_g)  # One graph\n",
        "    h = one_g.ndata['h']\n",
        "    one_g.ndata['h'] = self.gat(one_g, h)  # Update node embedding\n",
        "    output = dgl.mean_nodes(one_g, feat='h')\n",
        "    score = F.log_softmax(output, dim=-1)  # Score on each class\n",
        "\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frQwqZWSI5DE"
      },
      "source": [
        "#Model Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwSMS3LsGRux"
      },
      "outputs": [],
      "source": [
        "# Train Function\n",
        "def Train(model, epoch, train_X, train_y, dev_X, dev_y, device):\n",
        "  optimizer = th.optim.AdamW(model.parameters(), weight_decay=0.01, lr=0.001)\n",
        "  PATIENCE = 10  # Patience on dev set to stop training\n",
        "  no_improve = 0\n",
        "  best_acc = 0.0\n",
        "  for e in range(epoch):\n",
        "    improved = ''\n",
        "    model.train()\n",
        "    for batch_X, batch_y in zip(train_X, train_y):\n",
        "      batch_y = batch_y.to(device)\n",
        "      optimizer.zero_grad()  # # Clear gradient\n",
        "      score = model(batch_X)\n",
        "      loss = F.nll_loss(score, batch_y)\n",
        "      loss.backward()  # Derive gradient\n",
        "      optimizer.step()  # Update gradient\n",
        "    #train_acc = Dev(model, train_X, train_y, device)\n",
        "    dev_acc = Dev(model, dev_X, dev_y, device)\n",
        "    if dev_acc>best_acc:\n",
        "      best_acc = dev_acc\n",
        "      no_improve = 0\n",
        "      improved = '*'\n",
        "      th.save(model, f'/content/MyModel.pkl')\n",
        "    else: \n",
        "      no_improve+=1\n",
        "    print(f'Epoch={e+1}/{epoch}, Train_loss={loss.item():.4f}, Dev_acc={dev_acc:.4f}, {improved}')\n",
        "    if no_improve>=PATIENCE:\n",
        "      print(f'No improvement on dev set, early stopping')\n",
        "      break\n",
        "\n",
        "\n",
        "# Dev Function\n",
        "def Dev(model, X, y, device):\n",
        "  model.eval()\n",
        "  total = 0.0\n",
        "  correct = 0.0\n",
        "  for batch_X, batch_y in zip(X, y):\n",
        "    batch_y = batch_y.to(device)\n",
        "\n",
        "    with th.no_grad():\n",
        "      score = model(batch_X)\n",
        "      pred = th.argmax(score, dim=1)\n",
        "      correct += th.sum(pred==batch_y)\n",
        "      total += len(batch_y)\n",
        "\n",
        "  return th.div(correct, total)\n",
        "\n",
        "\n",
        "# Test Function (Same as Dev)\n",
        "def Test(model, X, y, device):\n",
        "  model.eval()\n",
        "  total = 0.0\n",
        "  correct = 0.0\n",
        "  for batch_X, batch_y in zip(X, y):\n",
        "    batch_y = batch_y.to(device)\n",
        "\n",
        "    with th.no_grad():\n",
        "      score = model(batch_X)\n",
        "      pred = th.argmax(score, dim=1)\n",
        "      correct += th.sum(pred==batch_y)\n",
        "      total += len(batch_y)\n",
        "\n",
        "  return th.div(correct, total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC7Ve5eYiwra"
      },
      "source": [
        "#Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBkkgI8zhpTC"
      },
      "outputs": [],
      "source": [
        "# Copy Data from Cloud Disk to Local Machine\n",
        "def CopyData():\n",
        "  if os.path.exists('/content/data')==False:\n",
        "    %cp -av '/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/data/'  '/content'\n",
        "  if os.path.exists('/content/glove.6B.300d.w2vformat.txt')==False:\n",
        "    %cp -av '/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/glove.6B/glove.6B.300d.w2vformat.txt'  '/content'\n",
        "\n",
        "\n",
        "# Batch the Dataset\n",
        "def Batch(X, y, batch_size=64, shuffle=False):\n",
        "  data = [(X[i], y[i]) for i in range(len(y))]\n",
        "  if shuffle:\n",
        "    random.shuffle(data)\n",
        "    X = [t for t,_ in data]  # Shuffled X\n",
        "    y = [t for _,t in data]  # Shuffled y\n",
        "  \n",
        "  batched_X = [X[i:i+batch_size] for i in range(0, len(data), batch_size)]\n",
        "  batched_y = [th.tensor(y[i:i+batch_size]) for i in range(0, len(data), batch_size)]\n",
        "  \n",
        "  return batched_X, batched_y\n",
        "\n",
        "\n",
        "# Save Experiment Result (Acuuracy and Running Time)\n",
        "def SaveResult(acc, time, dataset_name):\n",
        "  df1 = pd.DataFrame({\"Acc\": acc})\n",
        "  df2 = pd.DataFrame({\"Time\": time})\n",
        "\n",
        "  df1.to_csv(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/Acc-{dataset_name}.csv', index=False)\n",
        "  df2.to_csv(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/Time-{dataset_name}.csv', index=False)\n",
        "\n",
        "def MeanAcc(dataset_name):\n",
        "  acc_mean = []\n",
        "  acc_std = []\n",
        "  for name in dataset_name:\n",
        "    df = pd.read_csv(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/Acc-{name}.csv')\n",
        "    acc = df[\"Acc\"].iloc[0:]\n",
        "    acc_mean.append(np.mean(acc))\n",
        "    acc_std.append(np.std(acc))\n",
        "\n",
        "  return acc_mean, acc_std\n",
        "\n",
        "def MeanTime(dataset_name):\n",
        "  mean_time = []\n",
        "  for name in dataset_name:\n",
        "    df = pd.read_csv(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/Time-{name}.csv')\n",
        "    time = df[\"Time\"].iloc[0:]\n",
        "    mean_time.append(np.mean(time))\n",
        "    \n",
        "  return mean_time\n",
        "\n",
        "# Save Average Result for All Dataset\n",
        "def SaveTotalResult(dataset):\n",
        "  my_acc, my_std = MeanAcc(dataset)\n",
        "  my_time = MeanTime(dataset)\n",
        "  print(f\"acc_mean={my_acc}\")\n",
        "  print(f\"acc_std={my_std}\")\n",
        "  print(f\"time_mean(s)={my_time}\")\n",
        "  now_time = datetime.now()\n",
        "  t = re.sub(r' |:|\\.', '-', str(now_time))\n",
        "  df = pd.DataFrame({\"dataset\":dataset, \"acc_mean\":my_acc, \"acc_std\":my_std, \"time_mean(s)\":my_time})\n",
        "  df.to_csv(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/Total-result{t}.csv', index=False)\n",
        "\n",
        "# Set Training Device\n",
        "DEVICE = th.device('cuda:0' if th.cuda.is_available() else 'cpu')\n",
        "if th.cuda.is_available():\n",
        "  print(f'device: {DEVICE}')\n",
        "  print(f'name: {th.cuda.get_device_name(0)}')\n",
        "  print(f'='*50)\n",
        "\n",
        "# Set Constant\n",
        "shuffle_time = 2\n",
        "run_time = 3  # Run n times to get average\n",
        "epoch=50\n",
        "dataset = ['r8',  'r52', 'oh', 'mr', '20ng']\n",
        "\n",
        "CopyData()\n",
        "print(\"=\"*50)\n",
        "for NAME in dataset:\n",
        "  all_acc = []\n",
        "  consume_time = []\n",
        "  for s in range(shuffle_time):\n",
        "    train_text_tok, train_label_tok, dev_text_tok, dev_label_tok, test_text_tok, test_label_tok, num_class, vocab = MyDataset(NAME)\n",
        "    dev_X, dev_y = Batch(dev_text_tok, dev_label_tok)\n",
        "    test_X, test_y = Batch(test_text_tok, test_label_tok)\n",
        "    for i in range(run_time):\n",
        "      train_X, train_y = Batch(train_text_tok, train_label_tok, shuffle=True)  # Shuffle train dataset run n times to get average\n",
        "      t0 = time.time()\n",
        "      row_model = MyModel(vocab, 300, 300, num_class, device=DEVICE)\n",
        "      row_model.to(DEVICE)\n",
        "      Train(row_model, epoch, train_X, train_y, dev_X, dev_y, DEVICE)\n",
        "      trained_model = th.load(f'/content/MyModel.pkl', map_location=DEVICE)\n",
        "      acc = Test(trained_model, test_X, test_y, DEVICE).cpu().numpy()\n",
        "      print(f'{NAME}, ShuffleTime={s+1}/{shuffle_time}, RunTime={i+1}/{run_time}, Test_acc={acc:.4f}, Ave_Epoch_Time={(time.time()-t0)/epoch:.1f}s, All_Epoch_Time={(time.time()-t0)/60:.1f}m')\n",
        "      all_acc.append(acc)\n",
        "      consume_time.append(time.time()-t0)\n",
        "      print(\"=\"*50)\n",
        "    print(\"=\"*100)\n",
        "  SaveResult(all_acc, consume_time, NAME)\n",
        "\n",
        "SaveTotalResult(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5McduBi5Ae2J"
      },
      "source": [
        "#Result Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCWiAKrCUHtr"
      },
      "source": [
        "Experiment result should be read out from local disk. (In order to analyze it later.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhK0Ix1YUjdI"
      },
      "outputs": [],
      "source": [
        "def ShowAllResult():\n",
        "  all_acc_mean = []\n",
        "  all_file = os.listdir(f\"/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/\")\n",
        "  for t in all_file:\n",
        "    if t.startswith(\"Total\"):\n",
        "      df = pd.read_csv(f\"/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/{t}\")\n",
        "      all_acc_mean.append(df[\"acc_mean\"])\n",
        "      #break\n",
        "  \n",
        "  return all_acc_mean\n",
        "\n",
        "dataset = ['r8',  'r52', 'oh', 'mr', '20ng']     \n",
        "# Show All Results\n",
        "all_acc_mean = ShowAllResult()\n",
        "\n",
        "fig = plt.figure(figsize=(18,9),dpi=100)\n",
        "ax1 = fig.add_subplot(211)\n",
        "\n",
        "for i in range(len(all_acc_mean)):\n",
        "  ax1.plot(dataset, all_acc_mean[i], \"o-.\", label=f'{i+1}')\n",
        "  for a,b in zip(dataset, all_acc_mean[i]):\n",
        "    ax1.text(a, b, round(b, 4),fontsize=5)\n",
        "\n",
        "ax2 = fig.add_subplot(212)\n",
        "x_length = np.arange(len(dataset))\n",
        "bar_width = 0.2\n",
        "for i in range(len(all_acc_mean)):\n",
        "  x = x_length+i*bar_width\n",
        "  y = all_acc_mean[i]\n",
        "  ax2.bar(x, y, width=bar_width, label=f'{i+1}')\n",
        "  for a,b in zip(x, y):\n",
        "    ax2.text(a, b, round(b,4),ha=\"center\", fontsize=10)\n",
        "\n",
        "  \n",
        "plt.xticks(x_length, dataset)\n",
        "\n",
        "ax1.legend()\n",
        "ax2.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare Baseline Result"
      ],
      "metadata": {
        "id": "X32mumJHBGuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline Method\n",
        "TextGCN = [0.9707, 0.9356, 0.6836, 0.7674, 0.8634]\n",
        "TextLevelGCN = [0.978, 0.946, 0.6994, None, None]\n",
        "my_best_acc = [0.9, 0.9, 0.6, 0.7, 0.8]\n",
        "\n",
        "fig = plt.figure(figsize=(6,6),dpi=100)\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "ax.plot(dataset, TextGCN, '^-.', label=\"TextGCN\")\n",
        "ax.plot(dataset, my_best_acc, '<-.', label=\"MyModel\")\n",
        "ax.plot(dataset, TextLevelGCN, \">-.\", label=\"TextLevelGCN\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tf1dQrKKloxa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "17Bq6ZIVkmH63CqHmmrMUl0ElvX_uQeJA",
      "authorship_tag": "ABX9TyMveMxJmvo+wPjZ0CLtcrCv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}